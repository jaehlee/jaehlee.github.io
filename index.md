---
layout: default
title: Main
---
# **Jaehoon Lee**

<img align="right" style="float:center;padding:10px;" width="200" src="/image/image.jpeg">

Google Research, Brain Team

1600 Amphitheatre Parkway  
attn: jaehlee  
Mountain View, CA, 94043  

**E-Mail**: jaehlee at google dot com

[[LinkedIn]](https://www.linkedin.com/in/eejaehoon), [[Twitter]](https://twitter.com/hoonkp)

**Curriculum Vitae**: [CV](https://drive.google.com/file/d/1qz_duj-900YzKf8Dkww_LIVC1PLm93LH/view?usp=sharing)


I am a Senior Research Scientist at [Google Brain Team](https://research.google.com/teams/brain/) working on scientific understanding of deep neural networks. 
I was in second-year cohort of the [AI Residency](https://ai.google/research/join-us/ai-residency) program. 

Before joining Google in 2017, my main research focus was on theoretical high-energy physics. 
I was a postdoctoral researcher in the [Department of Physics & Astronomy](http://www.phas.ubc.ca/) at [University of British Columbia (UBC)](http://www.ubc.ca/) in the String Theory Group. 
Before that, I completed my PhD in [Center for Theoretical Physics (CTP)](http://ctp.lns.mit.edu/) at [MIT](http://web.mit.edu/) working on theoretical physics. 


## News

* [New!] Sep 2021: Our paper [Dataset Distillation with Infinitely Wide Convolutional Networks](https://arxiv.org/abs/2107.13034) is accepted at **NeurIPS 2021**!

* Feb 2021: Our new paper [Explaining Neural Scaling Laws](https://arxiv.org/abs/2102.06701) is on ArXiv! 

* Jan 2021: Our paper [Exploring the Uncertainty Properties of Neural Networks’ Implicit Priors in the Infinite-Width Limit](https://openreview.net/forum?id=MjvduJCsE4) is accepted at **ICLR 2021**!

* Jan 2021: Our paper [Dataset Meta-Learning from Kernel Ridge-Regression](https://openreview.net/forum?id=l-PrrQrK0QR) is accepted at **ICLR 2021**!


* Dec 2020: Our paper [Wide Neural Networks of Any Depth Evolve as Linear Models Under Gradient Descent
](https://arxiv.org/abs/1902.06720) is featured at [Special Issue of Journal of Statistical Mechanics: Theory and Experiment](https://iopscience.iop.org/issue/1742-5468/2020/12)


* Dec 2020: We presented our paper [Finite Versus Infinite Neural Networks: an Empirical Study](https://arxiv.org/abs/2007.15801) at **NeurIPS 2020**. Check out the [virtual conference site for pre-recorded talks.](https://neurips.cc/virtual/2020/protected/poster_ad086f59924fffe0773f8d0ca22ea712.html) [[poster@twitter]](https://twitter.com/hoonkp/status/1336885598720647168)

* Nov 2020: Our new paper [Towards NNGP-guided Neural Architecture Search](https://arxiv.org/abs/2011.06006) is on ArXiv! 

* Nov 2020: Our new paper [Dataset Meta-Learning from Kernel Ridge-Regression](https://arxiv.org/abs/2011.00050) is on ArXiv! 

* Oct 2020: Our paper [Exploring the Uncertainty Properties of Neural Networks’ Implicit Priors in the Infinite-Width Limit](https://arxiv.org/abs/2010.07355) is on ArXiv!  

* Sep 2020: Our paper [Finite Versus Infinite Neural Networks: an Empirical Study](https://arxiv.org/abs/2007.15801) is accepted at **NeurIPS 2020** as a **spotlight** presentation (280/9454 ~ 2.96%)!

* Jul 2020: Our new paper [Exploring the Uncertainty Properties of Neural Networks’ Implicit Priors in the Infinite-Width Limit](https://arxiv.org/abs/2010.07355) is accepted at **ICML 2020 Workshop on Uncertainty & Robustness in Deep Learning**.

* Apr 2020: Our paper [Neural Tangents: Fast and Easy Infinite Neural Networks in Python](https://arxiv.org/abs/1912.02803) is accpeted at **ICLR 2020** as spotlight.

* Dec 2019: Our paper [Wide Neural Networks of Any Depth Evolve as Linear Models Under Gradient Descent
](https://arxiv.org/abs/1902.06720) is accepted at **NeurIPS 2019**. We will be presenting on [Thu Dec 12th 10:45 AM -- 12:45 PM @ East Exhibition Hall B + C #175.](https://nips.cc/Conferences/2019/Schedule?showEvent=13916)

* Jul 2019: [Measuring the Effects of Data Parallelism on Neural Network Training
](http://jmlr.org/papers/volume20/18-789/18-789.pdf) is now published at JMLR!

* Jul 2019: I started a new role as research scientist at Google Research, Brain Team.

<!---
* Jul 2019: Thanks everyone who contributed and participated at the [ICML 2019 Workshop on Theoretical Physics for Deep Learning](https://sites.google.com/view/icml2019phys4dl). Talks and slides are now [available](https://sites.google.com/view/icml2019phys4dl/schedule?authuser=0).
--->

## Publication

[[Google Scholar]](https://scholar.google.com/citations?user=d3YhiooAAAAJ&hl=en) [[Semantic Scholar]](https://www.semanticscholar.org/author/Jaehoon-Lee/49685832) [[arXiv]](https://arxiv.org/a/lee_j_7.html) 


* **Dataset Distillation with Infinitely Wide Convolutional Networks**  
Timothy Nguyen, Roman Novak, Lechao Xiao, **Jaehoon Lee**  
Neural Information Processing Systems (NeurIPS), 2021 (to appear)      
[[arXiv: 2107.13034]](https://arxiv.org/abs/2107.13034)  [[code / dataset]](https://github.com/google-research/google-research/tree/master/kip)


* **Explaining Neural Scaling Laws**  
Yasaman Bahri\*, Ethan Dyer\*, Jared Kaplan\*, **Jaehoon Lee**\*, Utkarsh Sharma\*   
[[arXiv: 2102.06701]](https://arxiv.org/abs/2102.06701)


* **Towards NNGP-guided Neural Architecture Search**  
Daniel S. Park\*, **Jaehoon Lee**\*, Daiyi Peng, Yuan Cao, Jascha Sohl-Dickstein   
[[arXiv: 2011.06006]](https://arxiv.org/abs/2011.06006) [[code]](https://github.com/google-research/google-research/tree/master/nngp_nas) 


* **Dataset Meta-Learning from Kernel Ridge-Regression**  
Timothy Nguyen, Zhourong Chen, **Jaehoon Lee**  
[International Conference on Learning Representations (ICLR), 2021](https://openreview.net/forum?id=l-PrrQrK0QR)      
[[arXiv: 2011.00050]](https://arxiv.org/abs/2011.00050) 


* **Exploring the Uncertainty Properties of Neural Networks’ Implicit Priors in the Infinite-Width Limit**  
Ben Adlam\*, **Jaehoon Lee**\*, Lechao Xiao\*, Jeffrey Pennington and Jasper Snoek  
[International Conference on Learning Representations (ICLR), 2021](https://openreview.net/forum?id=MjvduJCsE4) (to appear)      
[ICML 2020 Workshop on Uncertainty & Robustness in Deep Learning](http://www.gatsby.ucl.ac.uk/~balaji/udl2020/accepted-papers/UDL2020-paper-115.pdf)
[[arXiv: 2010:07355]](https://arxiv.org/abs/2010.07355)  


* **Finite Versus Infinite Neural Networks: an Empirical Study**  
**Jaehoon Lee**, Samuel S. Schoenholz, Jeffrey Pennington, Ben Adlam, Lechao Xiao, Roman Novak, Jascha Sohl-Dickstein   
Neural Information Processing Systems (NeurIPS), 2020. [**spotlight**]    
[[arXiv: 2007.15801]](https://arxiv.org/abs/2007.15801). 


* **On the infinite width limit of neural networks with a standard parameterization**  
Jascha Sohl-Dickstein, Roman Novak, Samuel S. Schoenholz, **Jaehoon Lee**  
[[arXiv: 2001.07301]](https://arxiv.org/abs/2001.07301)

* **Neural Tangents: Fast and Easy Infinite Neural Networks in Python**  
Roman Novak, Lechao Xiao, Jiri Hron, **Jaehoon Lee**, Alexander A. Alemi, Jascha Sohl-Dickstein, Samuel S. Schoenholz 
[International Conference on Learning Representation(ICLR), 2020](https://openreview.net/forum?id=SklD9yrFPS) [**spotlight**]  
[[arXiv: 1912.02803]](https://arxiv.org/abs/1912.02803) [[code]](https://github.com/google/neural-tangents)

* **On Empirical Comparisons of Optimizers for Deep Learning**  
Dami Choi, Christopher J. Shallue, Zachary Nado, **Jaehoon Lee**, Chris J. Maddison, George E. Dahl  
[[arXiv: 1910.05446]](https://arxiv.org/abs/1910.05446)

* **Wide Neural Networks of Any Depth Evolve as Linear Models Under Gradient Descent**  
**Jaehoon Lee**\*, Lechao Xiao\*, Samuel S. Schoenholz, Yasaman Bahri, Jascha Sohl-Dickstein, Jeffrey Pennington  
[Neural Information Processing Systems (NeurIPS), 2019.](https://papers.nips.cc/paper/9063-wide-neural-networks-of-any-depth-evolve-as-linear-models-under-gradient-descent)  
[Special Isssue, Journal of Statistical Mechanics: Theory and Experiment, 2020.](https://iopscience.iop.org/article/10.1088/1742-5468/abc62b)  
[[arXiv: 1902.06720]](https://arxiv.org/abs/1902.06720) [[code1]](https://colab.research.google.com/github/google/neural-tangents/blob/master/notebooks/function_space_linearization.ipynb) [[code2]](https://colab.research.google.com/github/google/neural-tangents/blob/master/notebooks/weight_space_linearization.ipynb) [[Wikipedia(Neural tangent kernel)]](https://en.wikipedia.org/wiki/Neural_tangent_kernel) 


* **Measuring the Effects of Data Parallelism on Neural Network Training**  
Christopher J. Shallue\*, **Jaehoon Lee**\*, Joseph Antognini, Jascha Sohl-Dickstein, Roy Frostig, George E. Dahl  
[Journal of Machine Learning Research, 2019.](http://jmlr.org/papers/volume20/18-789/18-789.pdf)  
[[arXiv: 1811.03600]](https://arxiv.org/abs/1811.03600)

* **Bayesian Deep Convolutional Networks with Many Channels are Gaussian Processes**  
Roman Novak\*, Lechao Xiao\*, **Jaehoon Lee**&, Yasaman Bahri&, Greg Yang, Daniel A. Abolafia, Jeffrey Pennington, Jascha Sohl-Dickstein  
International Conference on Learning Representations (ICLR), 2019.  
[[arXiv: 1810.05148]](https://arxiv.org/abs/1810.05148)


* **Deep Neural Networks as Gaussian Processes**  
**Jaehoon Lee**\*, Yasaman Bahri\*, Roman Novak, Samuel S. Schoenholz, Jeffrey Pennington, Jascha Sohl-Dickstein  
International Conference on Learning Representations (ICLR), 2018.  
[[arXiv: 1711.00165]](https://arxiv.org/abs/1711.00165) [[code]](https://github.com/brain-research/nngp) [[Wikipedia(Neural network Gaussian process)]](https://en.wikipedia.org/wiki/Neural_network_Gaussian_process)

* **3d N=2 minimal SCFTs from Wrapped M5-branes**  
Jin-Beom Bae\*, Dongmin Gang\*, **Jaehoon Lee**\*  
Journal of High Energy Physics (JHEP), 2017.  
[[arXiv: 1610.09259]](https://arxiv.org/abs/1610.09259)

* **Linking dynamical heterogeneity to static amorphous order**  
Patrick Charbonneau\*, Ethan Dyer\*, **Jaehoon Lee**\*, Sho Yaida\*  
Journal of Statistical Mechanics: Theory and Experiment, 2016.  
[[arXiv: 1309.5085]](https://arxiv.org/abs/1309.5085)

* **Entanglement entropy from one-point functions in holographic states**  
Matthew J. S. Beach\*, **Jaehoon Lee**\*, Charles Rabideau\*, Mark Van Raamsdonk\*  
Journal of High Energy Physics (JHEP), 2016.  
[[arXiv: 1604.05308]](https://arxiv.org/abs/1604.05308)

* **Studies of superconformal field theories using GLSM and conformal bootstrap**  
**Jaehoon Lee**  
PhD Thesis, Massachusetts Institute of Technology  
[[Thesis]](https://dspace.mit.edu/handle/1721.1/99308)

* **Glassy slowdown and replica-symmetry-breaking instantons**  
Allan Adams\*, Tarek Anous\*, **Jaehoon Lee**\*, Sho Yaida\*  
Physical Review E (PRE), 2015.  
[[arXiv: 1406.1498]](https://arxiv.org/abs/1406.1498)

* **Exact Correlators of BPS Operators from the 3d Superconformal Bootstrap**  
Shai M. Chester\*, **Jaehoon Lee**\*, Silviu S. Pufu\*, Ran Yacoby\*   
Journal of High Energy Physics (JHEP), 2015.  
[[arXiv: 1412.0334]](https://arxiv.org/abs/1412.0334)

* **The N=8 Superconformal Bootstrap in Three Dimensions**  
Shai M. Chester\*, **Jaehoon Lee**\*, Silviu S. Pufu\*, Ran Yacoby\*  
Journal of High Energy Physics (JHEP), 2014.  
[[arXiv: 1406.4814]](https://arxiv.org/abs/1406.4814)

* **Algebra of Majorana Doubling**  
**Jaehoon Lee**\*, Frank Wilczek\*  
Physics Review Letters (PRL), 2013.  
[[arXiv: 1307.3245]](https://arxiv.org/abs/1307.3245)

* **GLSMs for non-Kahler Geometries**  
Allan Adams\*, Ethan Dyer\*, **Jaehoon Lee**\*  
Journal of High Energy Physics (JHEP), 2013.  
[[arXiv: 1206.5815]](https://arxiv.org/abs/1206.5815)

## Research

* Recent research interests include:
  1. Interplay between physics and machine learning
  2. Theoretical aspects of deep neural networks
  3. Scientific and principled study of deep neural networks and their learning algorithms
  4. Theoretical physics with focus on high energy physics
  

* Services:
  1. Area Chair for NeurIPS
  2. Reviewer for ICLR / ICML / NeurIPS / JMLR / Neural Computation / Pattern Recognition Letters / Nature Communications / TPAMI
  3. Organizer for [Aspen Winter Conference on Physics for Machine Learning](https://sites.google.com/corp/view/phys4ml/)
  4. Organizer for ICML Workshop on Theoretical Physics for Deep Learning
  5. Organizer for Vancouver deep learning study group
